{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Camp Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Read Below files. \n",
    "# First_Health_Camp_Attended.csv\n",
    "# Second_Health_Camp_Attended.csv\n",
    "# Third_Health_Camp_Attended.csv \n",
    "\n",
    "hcamp1 = sc.textFile(\"file:////Users/cdean4/Downloads/Train/First_Health_Camp_Attended.csv\")\n",
    "hcamp2 = sc.textFile(\"file:////Users/cdean4/Downloads/Train/Second_Health_Camp_Attended.csv\")\n",
    "hcamp3 = sc.textFile(\"file:////Users/cdean4/Downloads/Train/Third_Health_Camp_Attended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# Merge the 3 files into one data frame. \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df1 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"file:////Users/cdean4/Downloads/Train/First_Health_Camp_Attended.csv\")\n",
    "df2 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"file:////Users/cdean4/Downloads/Train/Second_Health_Camp_Attended.csv\")\n",
    "df3 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"file:////Users/cdean4/Downloads/Train/Third_Health_Camp_Attended.csv\")\n",
    "\n",
    "df1 = df1.withColumnRenamed(\"Health_Camp_ID\", \"Health_Camp_ID1\")\n",
    "df2 = df2.withColumnRenamed(\"Health_Camp_ID\", \"Health_Camp_ID2\")  \n",
    "df3 = df3.withColumnRenamed(\"Health_Camp_ID\", \"Health_Camp_ID3\")\n",
    "\n",
    "merged = df1.join(df2, [\"Patient_ID\"]).join(df3, [\"Patient_ID\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|\n",
      "+----------+---------------+--------+------------+----+\n",
      "|    506181|           6560|      40|  0.43902439|null|\n",
      "|    494977|           6560|      20| 0.097560976|null|\n",
      "|    518680|           6560|      10| 0.048780488|null|\n",
      "|    509916|           6560|      30| 0.634146341|null|\n",
      "|    488006|           6560|      20| 0.024390244|null|\n",
      "|    492080|           6560|      40| 0.658536585|null|\n",
      "|    521555|           6560|      30| 0.536585366|null|\n",
      "|    493258|           6560|      30|  0.12195122|null|\n",
      "|    515459|           6560|      20| 0.463414634|null|\n",
      "|    504261|           6560|      30|  0.87804878|null|\n",
      "|    496350|           6560|      50| 0.585365854|null|\n",
      "|    509797|           6560|      30| 0.707317073|null|\n",
      "|    487977|           6560|      40| 0.926829268|null|\n",
      "|    510500|           6560|      20| 0.073170732|null|\n",
      "|    505463|           6560|      70| 0.756097561|null|\n",
      "|    495620|           6560|      20| 0.512195122|null|\n",
      "|    497120|           6560|      30| 0.292682927|null|\n",
      "|    504227|           6560|      30| 0.804878049|null|\n",
      "|    511490|           6560|      20| 0.219512195|null|\n",
      "|    509188|           6560|      20| 0.170731707|null|\n",
      "+----------+---------------+--------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+---------------+------------+\n",
      "|Patient_ID|Health_Camp_ID2|Health Score|\n",
      "+----------+---------------+------------+\n",
      "|    526631|           6536| 0.875135722|\n",
      "|    509122|           6536| 0.755700326|\n",
      "|    498864|           6536| 0.673181325|\n",
      "|    515398|           6536|  0.72204126|\n",
      "|    504624|           6536| 0.464712269|\n",
      "|    486444|           6536| 0.587404995|\n",
      "|    513612|           6536|  0.72204126|\n",
      "|    493571|           6536| 0.825190011|\n",
      "|    494559|           6536| 0.963083605|\n",
      "|    490128|           6536| 0.755700326|\n",
      "|    495905|           6536|  0.72204126|\n",
      "|    505834|           6536| 0.897937025|\n",
      "|    509708|           6536|  0.52660152|\n",
      "|    491905|           6536| 0.159609121|\n",
      "|    512463|           6536| 0.755700326|\n",
      "|    495867|           6536|  0.72204126|\n",
      "|    497120|           6536| 0.241042345|\n",
      "|    491435|           6536| 0.499457112|\n",
      "|    490834|           6536| 0.825190011|\n",
      "|    493434|           6536| 0.825190011|\n",
      "+----------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+---------------+-----------------------+-------------------------+\n",
      "|Patient_ID|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|\n",
      "+----------+---------------+-----------------------+-------------------------+\n",
      "|    517875|           6527|                      3|                        1|\n",
      "|    504692|           6578|                      1|                        1|\n",
      "|    504692|           6527|                      3|                        1|\n",
      "|    493167|           6527|                      4|                        4|\n",
      "|    510954|           6528|                      2|                        2|\n",
      "|    501825|           6527|                      2|                        4|\n",
      "|    495620|           6527|                      1|                        1|\n",
      "|    526542|           6528|                      2|                        2|\n",
      "|    517351|           6578|                      3|                        5|\n",
      "|    517351|           6527|                      3|                        1|\n",
      "|    518855|           6541|                      1|                        1|\n",
      "|    518855|           6578|                      6|                        4|\n",
      "|    518855|           6527|                      6|                        2|\n",
      "|    486243|           6541|                      5|                        5|\n",
      "|    490447|           6528|                      3|                        2|\n",
      "|    521217|           6528|                      4|                        4|\n",
      "|    527017|           6528|                      4|                        2|\n",
      "|    527017|           6527|                      1|                        1|\n",
      "|    498186|           6527|                      6|                        6|\n",
      "|    528205|           6527|                      5|                        2|\n",
      "+----------+---------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+\n",
      "|    526631|           6537|      10| 0.031120332|null|           6536| 0.875135722|           6541|                      3|                        1|\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6527|                      2|                        2|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6578|                      5|                        5|\n",
      "|    498864|           6562|      10| 0.820224719|null|           6536| 0.673181325|           6578|                      1|                        1|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# Read the below files \n",
    "# Patient_Profile.csv\n",
    "# Health_Camp_Detail.csv\n",
    "\n",
    "pprof = sc.textFile(\"file:////Users/cdean4/Downloads/Train/Patient_Profile.csv\")\n",
    "hcampd = sc.textFile(\"file:////Users/cdean4/Downloads/Train/Health_Camp_Detail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Join the resultant from step 2 with Health_Camp_Detail.csv on Health_Camp_ID\n",
    "\n",
    "df4 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"file:////Users/cdean4/Downloads/Train/Health_Camp_Detail.csv\")\n",
    "df5 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"file:////Users/cdean4/Downloads/Train/Patient_Profile.csv\")\n",
    "\n",
    "df4 = df4.withColumnRenamed(\"Health_Camp_ID\", \"Health_Camp_ID1\")\n",
    "df5 = df5.withColumnRenamed(\"Health_Camp_ID\", \"Health_Camp_ID5\")\n",
    "merged2 = merged.join(df4, [\"Health_Camp_ID1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+\n",
      "|Health_Camp_ID1|Patient_ID|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|\n",
      "+---------------+----------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+\n",
      "|           6537|    526631|      10| 0.031120332|null|           6536| 0.875135722|           6541|                      3|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|\n",
      "|           6571|    509122|      10| 0.668367347|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Oct-05|    12-Oct-05|    First|        E|        2|\n",
      "|           6571|    509122|      10| 0.668367347|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Oct-05|    12-Oct-05|    First|        E|        2|\n",
      "|           6570|    509122|      30| 0.375451264|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Jul-05|    22-Jul-05|    First|        E|        2|\n",
      "|           6570|    509122|      30| 0.375451264|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Jul-05|    22-Jul-05|    First|        E|        2|\n",
      "|           6554|    509122|      20| 0.424855491|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Jun-05|    01-Jul-05|    First|        B|        2|\n",
      "|           6554|    509122|      20| 0.424855491|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Jun-05|    01-Jul-05|    First|        B|        2|\n",
      "|           6532|    509122|      30| 0.809160305|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|\n",
      "|           6532|    509122|      30| 0.809160305|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Feb-05|    23-Aug-05|    First|        F|        2|\n",
      "|           6542|    509122|      50| 0.897790055|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|\n",
      "+---------------+----------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|Maybe|Camp_Start_date|\n",
      "+-----+---------------+\n",
      "|null |27-Sep-05      |\n",
      "|null |09-Oct-05      |\n",
      "|null |09-Oct-05      |\n",
      "|null |09-Jul-05      |\n",
      "|null |09-Jul-05      |\n",
      "|null |19-Jun-05      |\n",
      "|null |19-Jun-05      |\n",
      "|null |19-Feb-05      |\n",
      "|null |19-Feb-05      |\n",
      "|null |19-Feb-05      |\n",
      "+-----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "# Find the patient response in days and add the column to resultant set of step 4. \n",
    "# patient response = Camp_Start_Date - First_Interaction\n",
    "\n",
    "# get camp_start_date and first_interaction in one df\n",
    "merged3 = merged2.join(df5, [\"Patient_ID\"])\n",
    "\n",
    "# convert date columns from strings to dates\n",
    "from pyspark.sql.types import DateType\n",
    "import datetime \n",
    "spark_dftest = merged3.withColumn(\"Maybe\",merged3['Camp_Start_Date'].cast(TimestampType()))\n",
    "#below is the result\n",
    "spark_dftest.select('Maybe','Camp_Start_date').show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second attemp to convert strings to dates\n",
    "import datetime\n",
    "\n",
    "def getTimeDif(startDate, firstInter):\n",
    "    sDate = datetime.datetime.strptime(startDate, \"%d-%m-%y\").date()\n",
    "    eDate = datetime.datetime.strptime(firstInter, \"%d-%m-%y\").date()\n",
    "    return str((sDate-eDate).days)+\" days\"\n",
    "\n",
    "getTimeDiffUDF = sql.functions.udf(getTimeDif)\n",
    "all_merged = merged3.withColumn(\"Patient_Response\", getTimeDiffUDF(merged3[\"Camp_Start_Date\"],merged3[\"First_Interaction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Patient_ID: integer (nullable = true)\n",
      " |-- Health_Camp_ID1: integer (nullable = true)\n",
      " |-- Donation: integer (nullable = true)\n",
      " |-- Health_Score: double (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- Health_Camp_ID2: integer (nullable = true)\n",
      " |-- Health Score: double (nullable = true)\n",
      " |-- Health_Camp_ID3: integer (nullable = true)\n",
      " |-- Number_of_stall_visited: integer (nullable = true)\n",
      " |-- Last_Stall_Visited_Number: integer (nullable = true)\n",
      " |-- Camp_Start_Date: string (nullable = true)\n",
      " |-- Camp_End_Date: string (nullable = true)\n",
      " |-- Category1: string (nullable = true)\n",
      " |-- Category2: string (nullable = true)\n",
      " |-- Category3: integer (nullable = true)\n",
      " |-- Online_Follower: integer (nullable = true)\n",
      " |-- LinkedIn_Shared: integer (nullable = true)\n",
      " |-- Twitter_Shared: integer (nullable = true)\n",
      " |-- Facebook_Shared: integer (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      " |-- Education_Score: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- First_Interaction: string (nullable = true)\n",
      " |-- City_Type: string (nullable = true)\n",
      " |-- Employer_Category: string (nullable = true)\n",
      " |-- Patient_Response: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2229.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 458.0 failed 1 times, most recent failure: Lost task 0.0 in stage 458.0 (TID 492, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-228-041d7868fb34>\", line 5, in getTimeDif\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 565, in _strptime_datetime\n    tt, fraction = _strptime(data_string, format)\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 362, in _strptime\n    (data_string, format))\nValueError: time data '27-Sep-05' does not match format '%d-%m-%y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-228-041d7868fb34>\", line 5, in getTimeDif\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 565, in _strptime_datetime\n    tt, fraction = _strptime(data_string, format)\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 362, in _strptime\n    (data_string, format))\nValueError: time data '27-Sep-05' does not match format '%d-%m-%y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-817c251c50ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2229.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 458.0 failed 1 times, most recent failure: Lost task 0.0 in stage 458.0 (TID 492, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-228-041d7868fb34>\", line 5, in getTimeDif\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 565, in _strptime_datetime\n    tt, fraction = _strptime(data_string, format)\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 362, in _strptime\n    (data_string, format))\nValueError: time data '27-Sep-05' does not match format '%d-%m-%y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/musondanyendwa/server/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-228-041d7868fb34>\", line 5, in getTimeDif\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 565, in _strptime_datetime\n    tt, fraction = _strptime(data_string, format)\n  File \"/anaconda3/lib/python3.6/_strptime.py\", line 362, in _strptime\n    (data_string, format))\nValueError: time data '27-Sep-05' does not match format '%d-%m-%y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "all_merged.printSchema()\n",
    "all_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6\n",
    "# Find the camp duration in days and add the column to resultant set of step 5. \n",
    "# Camp_Duration = Camp_End_Date - Camp_Start_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|Online_Follower|LinkedIn_Shared|Twitter_Shared|Facebook_Shared|Income|Education_Score|Age|First_Interaction|City_Type|Employer_Category|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Oct-05|    12-Oct-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Oct-05|    12-Oct-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Jul-05|    22-Jul-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Jul-05|    22-Jul-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6527|                      2|                        2|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6578|                      5|                        5|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6527|                      2|                        2|      01-Feb-04|    18-Feb-04|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6578|                      5|                        5|      01-Feb-04|    18-Feb-04|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Jan-04|    04-Feb-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Jan-04|    04-Feb-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6527|                      2|                        2|      07-Dec-03|    13-Jun-04|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6578|                      5|                        5|      07-Dec-03|    13-Jun-04|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    494559|           6554|      10|  0.12716763|null|           6536| 0.963083605|           6527|                      5|                        1|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    494559|           6526|      20| 0.592857143|null|           6536| 0.963083605|           6527|                      5|                        1|      03-Jan-05|    20-Feb-05|    First|        E|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 7\n",
    "# Delete all the entries where the City_Type = G\n",
    "\n",
    "#from pyspark.sql.functions import col\n",
    "\n",
    "noG_df = merged3.select('*').where(\"City_Type!='G'\")\n",
    "noG_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|Online_Follower|LinkedIn_Shared|Twitter_Shared|Facebook_Shared|Income|Education_Score|Age|First_Interaction|City_Type|Employer_Category|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Oct-05|    12-Oct-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6571|      10| 0.668367347|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Oct-05|    12-Oct-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Jul-05|    22-Jul-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6570|      30| 0.375451264|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Jul-05|    22-Jul-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6554|      20| 0.424855491|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6532|      30| 0.809160305|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6527|                      2|                        2|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6542|      50| 0.897790055|null|           6536| 0.755700326|           6578|                      5|                        5|      19-Feb-05|    23-Aug-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6527|                      2|                        2|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6580|      40| 0.203846154|null|           6536| 0.755700326|           6578|                      5|                        5|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6527|                      2|                        2|      01-Feb-04|    18-Feb-04|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6535|      50| 0.416666667|null|           6536| 0.755700326|           6578|                      5|                        5|      01-Feb-04|    18-Feb-04|    First|        E|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6527|                      2|                        2|      09-Jan-04|    04-Feb-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6538|      20| 0.039936102|null|           6536| 0.755700326|           6578|                      5|                        5|      09-Jan-04|    04-Feb-05|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6527|                      2|                        2|      07-Dec-03|    13-Jun-04|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    509122|           6581|      30| 0.539325843|null|           6536| 0.755700326|           6578|                      5|                        5|      07-Dec-03|    13-Jun-04|    First|        F|        2|              0|              1|             1|              1|     2|             80| 41|        24-Aug-03|        H|           Others|\n",
      "|    494559|           6554|      10|  0.12716763|null|           6536| 0.963083605|           6527|                      5|                        1|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    494559|           6526|      20| 0.592857143|null|           6536| 0.963083605|           6527|                      5|                        1|      03-Jan-05|    20-Feb-05|    First|        E|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 8\n",
    "# If Number_of_stall_visited is 1 then remove all the records pertaining to the respective Patient_ID. \n",
    "new_df = noG_df.select('*').where(\"Number_of_stall_visited!=1\")\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|Online_Follower|LinkedIn_Shared|Twitter_Shared|Facebook_Shared|Income|Education_Score|Age|First_Interaction|City_Type|Employer_Category|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|    508608|           6561|      90| 0.563636364|null|           6536| 0.421281216|           6578|                      2|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     6|             81| 59|        20-Oct-03|        B|       Consulting|\n",
      "|    524200|           6561|      10| 0.109090909|null|           6534| 0.886650869|           6528|                      2|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             1|              1|     0|           None| 65|        12-Nov-03|        D|       Technology|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6549| 0.937163375|           6527|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    494977|           6561|      10| 0.745454545|null|           6536| 0.673181325|           6578|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             1|              0|     2|             68| 50|        12-Aug-03|        H|        Transport|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6549| 0.937163375|           6578|                      3|                        4|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6536| 0.978284473|           6578|                      3|                        4|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    494977|           6561|      10| 0.745454545|null|           6555| 0.615827338|           6527|                      2|                        1|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             1|              0|     2|             68| 50|        12-Aug-03|        H|        Transport|\n",
      "|    494977|           6561|      10| 0.745454545|null|           6536| 0.673181325|           6527|                      2|                        1|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             1|              0|     2|             68| 50|        12-Aug-03|        H|        Transport|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6536| 0.978284473|           6527|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    494977|           6561|      10| 0.745454545|null|           6555| 0.615827338|           6578|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             1|              0|     2|             68| 50|        12-Aug-03|        H|        Transport|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6555| 0.824460432|           6527|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6555| 0.824460432|           6578|                      3|                        4|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    508608|           6561|      90| 0.563636364|null|           6529| 0.161640531|           6578|                      2|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     6|             81| 59|        20-Oct-03|        B|       Consulting|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6529| 0.884800965|           6527|                      5|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    509188|           6561|      20| 0.745454545|null|           6529| 0.884800965|           6578|                      3|                        4|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     1|             67| 56|        06-Mar-03|        F|       Technology|\n",
      "|    528219|           6561|     100| 0.909090909|null|           6536| 0.421281216|           6527|                      2|                        2|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     5|             86| 56|        05-Oct-03|        B|        Education|\n",
      "|    528219|           6561|     100| 0.909090909|null|           6536| 0.421281216|           6578|                      3|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     5|             86| 56|        05-Oct-03|        B|        Education|\n",
      "|    493870|           6561|      10| 0.963636364|null|           6523| 0.987012987|           6527|                      5|                        5|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     0|           None| 42|        29-May-03|        C|           Health|\n",
      "|    493870|           6561|      10| 0.963636364|null|           6523| 0.987012987|           6578|                      5|                        2|      30-Nov-03|    18-Dec-03|    First|        E|        1|              0|              0|             0|              0|     0|           None| 42|        29-May-03|        C|           Health|\n",
      "|    490407|           6561|      70| 0.781818182|null|           6549| 0.113105925|           6527|                      2|                        3|      30-Nov-03|    18-Dec-03|    First|        E|        1|              1|              1|             0|              1|     1|           None| 74|        14-Nov-03|        I|           Others|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 9\n",
    "# Sort the data based on Camp_Start_Date\n",
    "#new_df.count().filter(\"'count'>=10\").sort(desc(\"count\"))\n",
    "from pyspark.sql.functions import desc\n",
    "new_sort = new_df.sort(desc(\"Camp_Start_Date\"))\n",
    "new_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|Patient_ID|Health_Camp_ID1|Donation|Health_Score| _c4|Health_Camp_ID2|Health Score|Health_Camp_ID3|Number_of_stall_visited|Last_Stall_Visited_Number|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|Online_Follower|LinkedIn_Shared|Twitter_Shared|Facebook_Shared|Income|Education_Score|Age|First_Interaction|City_Type|Employer_Category|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "|    494559|           6554|      10|  0.12716763|null|           6536| 0.963083605|           6527|                      5|                        1|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    494559|           6526|      20| 0.592857143|null|           6536| 0.963083605|           6527|                      5|                        1|      03-Jan-05|    20-Feb-05|    First|        E|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    494559|           6580|      50| 0.776923077|null|           6536| 0.963083605|           6527|                      5|                        1|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    494559|           6540|      10| 0.542857143|null|           6536| 0.963083605|           6527|                      5|                        1|      01-Nov-04|    04-Nov-04|    First|        E|        2|              0|              0|             0|              0|     1|             66| 49|        08-Feb-03|        H|             null|\n",
      "|    520196|           6571|      20|  0.12755102|null|           6536| 0.875135722|           6527|                      6|                        5|      09-Oct-05|    12-Oct-05|    First|        E|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6554|      20| 0.052023121|null|           6536| 0.875135722|           6527|                      6|                        5|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6526|      20| 0.435714286|null|           6536| 0.875135722|           6527|                      6|                        5|      03-Jan-05|    20-Feb-05|    First|        E|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6580|      40| 0.626923077|null|           6536| 0.875135722|           6527|                      6|                        5|      22-Dec-04|    06-Jan-05|    First|        E|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6562|      30| 0.494382022|null|           6536| 0.875135722|           6527|                      6|                        5|      24-Nov-04|    02-Jun-05|    First|        F|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6540|      50| 0.521428571|null|           6536| 0.875135722|           6527|                      6|                        5|      01-Nov-04|    04-Nov-04|    First|        E|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    520196|           6538|      50|  0.46485623|null|           6536| 0.875135722|           6527|                      6|                        5|      09-Jan-04|    04-Feb-05|    First|        F|        2|              0|              0|             0|              0|     0|             80| 44|        04-Jul-03|        E|Software Industry|\n",
      "|    522928|           6537|      10| 0.022821577|null|           6536| 0.421281216|           6527|                      6|                        4|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6537|      10| 0.022821577|null|           6536| 0.421281216|           6578|                      3|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6537|      10| 0.022821577|null|           6536| 0.421281216|           6541|                      1|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6537|      10| 0.022821577|null|           6536| 0.421281216|           6528|                      1|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6543|      30| 0.559854897|null|           6536| 0.421281216|           6527|                      6|                        4|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6543|      30| 0.559854897|null|           6536| 0.421281216|           6578|                      3|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6543|      30| 0.559854897|null|           6536| 0.421281216|           6541|                      1|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6543|      30| 0.559854897|null|           6536| 0.421281216|           6528|                      1|                        1|      27-Sep-05|    07-Nov-07|    First|        F|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "|    522928|           6554|      10| 0.080924855|null|           6536| 0.421281216|           6527|                      6|                        4|      19-Jun-05|    01-Jul-05|    First|        B|        2|              0|              0|             0|              0|     1|             66| 41|        25-Jun-04|        E|           Others|\n",
      "+----------+---------------+--------+------------+----+---------------+------------+---------------+-----------------------+-------------------------+---------------+-------------+---------+---------+---------+---------------+---------------+--------------+---------------+------+---------------+---+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 10\n",
    "# Similar to the step of 8 if the Online_Follower and LinkedIn_Shared \n",
    "# and Twitter_Shared and Facebook_Shared are 1 then romove all the records pertaining to that patient id. \n",
    "\n",
    "final_df = noG_df.select('*').where(\"Online_Follower!=1 AND LinkedIn_Shared!=1 AND Twitter_Shared!=1 AND Facebook_Shared!=1\")\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 11\n",
    "# Sort the output in CSV file. \n",
    "final_df.write.csv('Final.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
